# -*- coding: utf-8 -*-
"""AanalDobariya_AI_TA2Assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13qRJ0T9-6zxJ92-GSzcSOvJ1SVdGWeg7

DATASET LOADING
"""

# ---- 1. DATASET LOADING ----
import pandas as pd

# Load dataset
df = pd.read_csv("cybersecurity_intrusion_data.csv")
print("Dataset shape:", df.shape)
print("First 5 rows:")
print(df.head())

"""DATA CLEANING"""

# ---- 2. DATA CLEANING ----
# Drop irrelevant columns like session_id
if "session_id" in df.columns:
    df.drop(columns=["session_id"], inplace=True)

# Check for missing values
print(df.isnull().sum())

# Drop duplicates if any
df.drop_duplicates(inplace=True)

# Handle missing values (example: fill numerical with median, categorical with mode)
num_cols = df.select_dtypes(include=["int64","float64"]).columns
cat_cols = df.select_dtypes(include=["object","category"]).columns

for col in num_cols:
    df[col].fillna(df[col].median(), inplace=True)

for col in cat_cols:
    df[col].fillna(df[col].mode()[0], inplace=True)

print("After cleaning, dataset shape:", df.shape)

"""DATA PREPROCESSING"""

# ---- 3. DATA PREPROCESSING ----
from sklearn.preprocessing import LabelEncoder, StandardScaler
from imblearn.over_sampling import SMOTE

# Separate features and target
X = df.drop(columns=["attack_detected"])  # target column
y = df["attack_detected"]

# Redefine numerical & categorical columns based on X only
num_cols = X.select_dtypes(include=["int64","float64"]).columns.tolist()
cat_cols = X.select_dtypes(include=["object","category"]).columns.tolist()

# Encode categorical columns
for col in cat_cols:
    X[col] = LabelEncoder().fit_transform(X[col])

# Scale numerical features
scaler = StandardScaler()
X[num_cols] = scaler.fit_transform(X[num_cols])

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)

print("After SMOTE, class distribution:")
print(y_res.value_counts())

"""EDA"""

# ---- 4. EDA ----
import matplotlib.pyplot as plt
import seaborn as sns

# Bar plots for categorical columns
for col in cat_cols:
    plt.figure(figsize=(6,4))
    sns.countplot(x=col, data=df)
    plt.title(f"Count plot of {col}")
    plt.show()

# Correlation heatmap for numerical features
plt.figure(figsize=(10,8))
sns.heatmap(df[num_cols].corr(), annot=True, cmap="coolwarm")
plt.title("Correlation Heatmap of Numerical Features")
plt.show()

"""TRAIN-TEST SPLIT"""

# ---- 5. TRAIN-TEST SPLIT ----
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X_res, y_res, test_size=0.3, random_state=42, stratify=y_res
)

# Save train/test data
X_train.to_csv("X_train.csv", index=False)
X_test.to_csv("X_test.csv", index=False)
y_train.to_csv("y_train.csv", index=False)
y_test.to_csv("y_test.csv", index=False)

print("Train and test sets saved successfully.")

"""MODEL TRAINING"""

# ---- 6. MODEL TRAINING ----
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

# Initialize models
pipe_lr = LogisticRegression(max_iter=1000, random_state=42)
pipe_svc = SVC(probability=True, random_state=42)
pipe_rf = RandomForestClassifier(n_estimators=100, random_state=42)
pipe_xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Train models
pipe_lr.fit(X_train, y_train)
pipe_svc.fit(X_train, y_train)
pipe_rf.fit(X_train, y_train)
pipe_xgb.fit(X_train, y_train)

"""HYPERPARAMETER TUNING"""

# ---- 6.1 HYPERPARAMETER TUNING ----
from sklearn.model_selection import RandomizedSearchCV

# Random Forest
rf_param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
rand_rf = RandomizedSearchCV(pipe_rf, rf_param_dist, n_iter=10, cv=3, scoring='f1', n_jobs=-1, random_state=42)
rand_rf.fit(X_train, y_train)
pipe_rf = rand_rf.best_estimator_
print("Best RF params:", rand_rf.best_params_)

# XGBoost
xgb_param_dist = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.7, 0.8, 1.0]
}
rand_xgb = RandomizedSearchCV(pipe_xgb, xgb_param_dist, n_iter=10, cv=3, scoring='f1', n_jobs=-1, random_state=42)
rand_xgb.fit(X_train, y_train)
pipe_xgb = rand_xgb.best_estimator_
print("Best XGB params:", rand_xgb.best_params_)

# SVM
svc_param_dist = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}
rand_svc = RandomizedSearchCV(pipe_svc, svc_param_dist, n_iter=5, cv=3, scoring='f1', n_jobs=-1, random_state=42)
rand_svc.fit(X_train, y_train)
pipe_svc = rand_svc.best_estimator_
print("Best SVC params:", rand_svc.best_params_)

"""FEATURE IMPORTANCE"""

# ---- 7. FEATURE IMPORTANCE ----
importances_rf = pipe_rf.feature_importances_
importances_xgb = pipe_xgb.feature_importances_

feat_names = X_train.columns

# Random Forest
plt.figure(figsize=(10,6))
sns.barplot(x=importances_rf, y=feat_names)
plt.title("Random Forest Feature Importance")
plt.show()

# XGBoost
plt.figure(figsize=(10,6))
sns.barplot(x=importances_xgb, y=feat_names)
plt.title("XGBoost Feature Importance")
plt.show()

# ---- 7.1 FEATURE SELECTION BASED ON IMPORTANCE ----
import numpy as np

# Example for Random Forest
feat_importances = pipe_rf.feature_importances_
importance_threshold = 0.01  # drop features with importance < 1%
selected_features = X_train.columns[feat_importances > importance_threshold]

# Reduce X_train and X_test to selected features
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

print("Selected features after RF importance filtering:")
print(selected_features)

# Retrain Random Forest on selected features
pipe_rf.fit(X_train_selected, y_train)
# Similarly, retrain XGBoost if desired
pipe_xgb.fit(X_train_selected, y_train)

# ---- AFTER FEATURE SELECTION ----
# Use only the selected features for both training and testing
X_train_selected = X_train[selected_features]
X_test_selected = X_test[selected_features]

# Retrain models on selected features
pipe_rf.fit(X_train_selected, y_train)
pipe_xgb.fit(X_train_selected, y_train)
pipe_lr.fit(X_train_selected, y_train)
pipe_svc.fit(X_train_selected, y_train)

"""EVALUATION METRICS"""

# ---- 8. EVALUATION METRICS (UPDATED) ----
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, RocCurveDisplay
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Ensure test data has the same selected features
# 'selected_features' is from Step 7.1 Feature Selection
X_test_selected = X_test[selected_features]

# Models dictionary
models = {
    "Logistic Regression": pipe_lr,
    "SVM": pipe_svc,
    "Random Forest": pipe_rf,
    "XGBoost": pipe_xgb
}

# List to store metrics
metrics_list = []

for name, model in models.items():
    # Predict
    y_pred = model.predict(X_test_selected)

    # Get probabilities or decision function for ROC AUC
    if hasattr(model, "predict_proba"):
        y_proba = model.predict_proba(X_test_selected)[:,1]
    else:
        y_proba = model.decision_function(X_test_selected)

    # Store metrics
    metrics_list.append({
        "Model": name,
        "Accuracy": accuracy_score(y_test, y_pred),
        "Precision": precision_score(y_test, y_pred),
        "Recall": recall_score(y_test, y_pred),
        "F1-Score": f1_score(y_test, y_pred),
        "ROC AUC": roc_auc_score(y_test, y_proba)
    })

    # Confusion matrix
    plt.figure(figsize=(5,4))
    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues")
    plt.title(f"Confusion Matrix - {name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    # ROC Curve
    RocCurveDisplay.from_predictions(y_test, y_proba)
    plt.title(f"ROC Curve - {name}")
    plt.show()

# Convert to DataFrame
metrics_df = pd.DataFrame(metrics_list)
metrics_df.set_index("Model", inplace=True)

# Comparison chart
metrics_df.plot(kind="bar", figsize=(10,6))
plt.title("Comparison of ML Models - Evaluation Metrics")
plt.ylabel("Score")
plt.ylim(0,1)
plt.xticks(rotation=0)
plt.legend(loc="lower right")
plt.show()

# Print metrics table
print(metrics_df)

"""SAVING XGBOOST MODEL IN PICKLE FILE AND PREDICTING IF THE ATTACK HAPPENED OR NOT"""

import os
import pickle
import pandas as pd

# ---- 1. Define file paths ----
model_file = "xgb_model.pkl"
features_file = "selected_features.pkl"

# ---- 2. Save model and selected features if not found ----
if not os.path.exists(model_file) or not os.path.exists(features_file):
    print("Pickle files not found. Saving the current trained model and features...")
    # Replace pipe_xgb and selected_features with your trained variables
    with open(model_file, "wb") as f:
        pickle.dump(pipe_xgb, f)
    with open(features_file, "wb") as f:
        pickle.dump(selected_features.tolist(), f)
    print("Files saved successfully.")

# ---- 3. Load model and selected features ----
with open(model_file, "rb") as f:
    xgb_model = pickle.load(f)

with open(features_file, "rb") as f:
    selected_features = pickle.load(f)

# ---- 4. Interactive input function ----
def interactive_attack_prediction():
    print("Enter values for the following features:")

    input_data = {}
    for feature in selected_features:
        while True:
            try:
                value = float(input(f"{feature}: "))
                input_data[feature] = value
                break
            except ValueError:
                print("Please enter a numeric value.")

    # Convert to DataFrame
    input_df = pd.DataFrame([input_data])

    # Predict
    prediction = xgb_model.predict(input_df)[0]
    probability = xgb_model.predict_proba(input_df)[0][1]

    print("\n--- Prediction Result ---")
    print(f"Attack Detected: {'Yes' if prediction == 1 else 'No'}")
    print(f"Probability of Attack: {probability:.4f}")

# ---- 5. Run the interactive function ----
interactive_attack_prediction()